{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression using single and multiple variables\n",
    "#Important point: The learning rate is influenced by the number of samples and the values of the samples\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "##Class to implement Linear Regression\n",
    "class LinearRegression:\n",
    "    def __init__(self,samples,features,learning_rate,epochs,noise):\n",
    "        self.X_samples = samples              # Number of Samples = self.X_samples\n",
    "        self.X_features = features            # Number of features = self.X_features\n",
    "        self.learning_rate = learning_rate    # Learning Rate = self.learning_rate\n",
    "        self.epochs = epochs                  # Iterations = self.epochs\n",
    "        self.noise = noise                    # Influence of noise = self.noise \n",
    "    \n",
    "    ##Create a random Data sample for X using the number of samples and the number of features  \n",
    "    def XSamples(self):\n",
    "        return np.round(np.random.uniform(0,1,(self.X_samples,self.X_features)),2)\n",
    "    \n",
    "    ##Create thw output samples for the input samples.\n",
    "    ## y = w1x1+w2x2+w3x3+......wnxn+b (Equation of linear Reression)\n",
    "    ##Use random weight values to obtain the values of [w1,w2,w3,...,wn] for the output y\n",
    "    def ySamples(self,X):\n",
    "        temp = np.zeros(X.shape)\n",
    "        for i in range(self.X_features):\n",
    "            temp[:,i] = random.randint(1,8)*X[:,i]\n",
    "        y = np.array([np.sum(temp,axis=1)]).T+np.array(random.randint(1,10))\n",
    "        return y+self.noise*np.random.random((y.shape))\n",
    "    \n",
    "    ##Calcluate the value of linear regression variable for training\n",
    "    ## z = Xw+b - Equation of the regression variable\n",
    "    def forwardPropagation(self,X,w,b):\n",
    "        return np.dot(X,w.T)+b\n",
    "    \n",
    "    ##Find the derivatives for back propagation\n",
    "    ## dL = dL/dz, dz = dz/dw\n",
    "    ## dL/dw = (dL/dz)*(dz/dw)\n",
    "    ## dL/db = (dl/dz)*(dz/db)\n",
    "    def derivative(self,X,y,z):\n",
    "        dL = (y-z)/self.X_samples\n",
    "        dz = X\n",
    "        return (dL,dz)\n",
    "    \n",
    "    ##Update the weights after each iteration\n",
    "    ## dw = dL/dw\n",
    "    ## db = dL/db\n",
    "    def backwardPropagation(self,dL,dz):\n",
    "    # print(dJ.shape,dz_w.shape,dz_b.shape)\n",
    "        dw = np.dot(dL.T,dz)\n",
    "    # print(dw.shape)\n",
    "        db = np.sum(dL)\n",
    "    # print(db.shape)\n",
    "        return (dw,db)\n",
    "    \n",
    "    ##Initialize the weights and bias to any random values\n",
    "    def randomWeightsandBias(self,X):\n",
    "        w = np.random.uniform(-1,1,(1,X.shape[1]))\n",
    "        b = np.random.uniform(-1,1,(1,1))\n",
    "        return (w,b)\n",
    "    \n",
    "    ##Train the model\n",
    "    def train_model(self):\n",
    "        X = self.XSamples()                                     ##Get the input values\n",
    "        y = self.ySamples(X)                                    ##Get output values corresponding to input for linear regression\n",
    "        (w,b) = self.randomWeightsandBias(X)                    ##Get the random weights and bias values\n",
    "        ##Training the model over epochs number of iterations\n",
    "        for i in range(self.epochs):\n",
    "            z = self.forwardPropagation(X,w,b)                  ##Calculate thw regression variable after each iterations\n",
    "            if i%100 == 0:\n",
    "                loss = np.sum(np.square(y-z))/self.X_samples\n",
    "                print('Loss at',i,'th epoch =',loss)            ##Print the loss values after 100 iterations\n",
    "            (dL,dz) = self.derivative(X,y,z)                    ##Derivativecs for back propagations\n",
    "            (dw,db) = self.backwardPropagation(dL,dz)                 ##Get dw and db\n",
    "            w = w+self.learning_rate*dw                         ##Update weights after each iteration\n",
    "            b = b+self.learning_rate*db                         ##Update bias after each iteration\n",
    "        print(w,b)                                              ##Print final weights and bias values\n",
    "        if self.X_features == 1:                                ##Plot graph if only one feature \n",
    "            z = np.ravel(w*X+b)\n",
    "            plt.scatter(np.ravel(X),y)\n",
    "            plt.plot(np.ravel(X),z,color = 'red')\n",
    "\n",
    "##Instantiate the object\n",
    "lr = LinearRegression(samples=100,features=5,\n",
    "                      learning_rate=0.1,\n",
    "                      epochs=5000,noise=0)\n",
    "lr.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
